{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO/CGNA4fVDiA1esR4URFL5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/puneet170191/Roadmap-To-Learn-Generative-AI-In-2024/blob/main/V1_DeepSPEC_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lwkzk4A7tFsB",
        "outputId": "b7382717-cfa1-43d4-c6c9-9eb4f67360af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyWavelets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5U21vmg4H3w",
        "outputId": "b7325b98-d46e-44b7-9e9e-29e69dbcabe4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiscale CNN wtih PHysics informed features no transformer"
      ],
      "metadata": {
        "id": "I2xGYLEA4BQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DeepSPEC: Hybrid Speckle Classification Model with Per-Sample Physics Guidance\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from einops import rearrange\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Root directory and preview\n",
        "root_dir = \"/content/drive/My Drive/00_DeepSPEC/DataSet_Learning/\"\n",
        "print(\"Classes:\", os.listdir(root_dir))\n",
        "for class_id in range(7):\n",
        "    class_dir = os.path.join(root_dir, str(class_id))\n",
        "    sample_imgs = glob(f\"{class_dir}/**/*.tiff\", recursive=True)\n",
        "    print(f\"Class {class_id} has {len(sample_imgs)} images\")\n",
        "\n",
        "# Utility: Compute Speckle Features (mocked for now)\n",
        "def compute_speckle_features(img_tensor):\n",
        "    return torch.rand(16)  # Placeholder for real speckle feature extraction\n",
        "\n",
        "# 1. Multi-Scale CNN with 4 filters\n",
        "class MultiScaleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv_gaussian = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.conv_laplacian = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.conv_wavelet = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
        "        self.conv_atrous = nn.Conv2d(1, 8, kernel_size=3, padding=2, dilation=2)\n",
        "        self.norm = nn.BatchNorm2d(32)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        g = self.conv_gaussian(x)\n",
        "        l = self.conv_laplacian(x)\n",
        "        w = self.conv_wavelet(x)\n",
        "        a = self.conv_atrous(x)\n",
        "        out = torch.cat([g, l, w, a], dim=1)\n",
        "        out = self.relu(self.norm(out))\n",
        "        return out\n",
        "\n",
        "# 2. Patch Embedding Layer\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, in_channels=32, patch_size=50, emb_dim=128):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(in_channels, emb_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "# 3. Physics-Informed Transformer Block\n",
        "class PhysicsTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, nhead=4):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=nhead, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.ff = nn.Sequential(nn.Linear(dim, dim * 4), nn.ReLU(), nn.Linear(dim * 4, dim))\n",
        "\n",
        "    def forward(self, x, physics_embedding):\n",
        "        physics_embedding = physics_embedding.unsqueeze(1).expand(-1, x.size(1), -1)\n",
        "        x = x + physics_embedding\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        x = self.norm2(x + self.ff(x))\n",
        "        return x\n",
        "\n",
        "# 4. DeepSPEC Model\n",
        "class DeepSPEC(nn.Module):\n",
        "    def __init__(self, num_classes=7):\n",
        "        super().__init__()\n",
        "        self.phys_projector = nn.Linear(16, 128)\n",
        "        self.cnn = MultiScaleCNN()\n",
        "        self.patch_embed = PatchEmbed()\n",
        "        self.transformer = PhysicsTransformerBlock(128)\n",
        "        self.classifier = nn.Sequential(nn.LayerNorm(128), nn.Linear(128, num_classes))\n",
        "\n",
        "    def forward(self, x, physics_feat):\n",
        "        cnn_feat = self.cnn(x)\n",
        "        patches = self.patch_embed(cnn_feat)\n",
        "        physics_emb = self.phys_projector(physics_feat)\n",
        "        transformer_out = self.transformer(patches, physics_emb)\n",
        "        cls_token = transformer_out.mean(dim=1)\n",
        "        out = self.classifier(cls_token)\n",
        "        return out, physics_feat\n",
        "\n",
        "# 5. Dataset with per-sample feature extraction\n",
        "class DeepSpecDataset(Dataset):\n",
        "    def __init__(self, root_dir, img_size=350, samples_per_class=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.img_size = img_size\n",
        "        self.images_path = []\n",
        "        self.labels = []\n",
        "        class_folders = sorted(glob(f\"{root_dir}/*\"))\n",
        "        for class_path in class_folders:\n",
        "            class_idx = int(os.path.basename(class_path))\n",
        "            sub_folders = glob(f\"{class_path}/*\")\n",
        "            class_images = []\n",
        "            for sub_folder in sub_folders:\n",
        "                class_images.extend(glob(f\"{sub_folder}/*.tiff\"))\n",
        "            if samples_per_class:\n",
        "                class_images = random.sample(class_images, min(samples_per_class, len(class_images)))\n",
        "            self.images_path.extend(class_images)\n",
        "            self.labels.extend([class_idx] * len(class_images))\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Grayscale(),\n",
        "            transforms.Resize((img_size, img_size)),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_path)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.images_path[idx])\n",
        "        img_tensor = self.transform(img)\n",
        "        label = self.labels[idx]\n",
        "        physics_feat = compute_speckle_features(img_tensor)\n",
        "        return img_tensor, label, physics_feat\n",
        "\n",
        "# 6. Loss Function\n",
        "\n",
        "def hybrid_loss(outputs, targets, physics_feats, lambda_reg=0.01):\n",
        "    ce_loss = F.cross_entropy(outputs, targets)\n",
        "    phys_reg = torch.mean(torch.abs(physics_feats))\n",
        "    return ce_loss + lambda_reg * phys_reg\n",
        "\n",
        "# 7. Training Loop\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device='cuda'):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.to(device)\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        model.train()\n",
        "        total, correct, total_loss = 0, 0, 0.0\n",
        "        for imgs, labels, physics_feats in train_loader:\n",
        "            imgs, labels, physics_feats = imgs.to(device), labels.to(device), physics_feats.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs, phys = model(imgs, physics_feats)\n",
        "            loss = hybrid_loss(outputs, labels, phys)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            preds = outputs.argmax(dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100. * correct / total\n",
        "        print(f\"Epoch {ep+1}/{epochs}: Train Loss={total_loss/len(train_loader):.4f}, Train Acc={train_acc:.2f}%\")\n",
        "\n",
        "        model.eval()\n",
        "        val_correct, val_total, val_loss = 0, 0, 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels, physics_feats in val_loader:\n",
        "                imgs, labels, physics_feats = imgs.to(device), labels.to(device), physics_feats.to(device)\n",
        "                outputs, phys = model(imgs, physics_feats)\n",
        "                loss = hybrid_loss(outputs, labels, phys)\n",
        "                val_loss += loss.item()\n",
        "                preds = outputs.argmax(dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = 100. * val_correct / val_total\n",
        "        print(f\"Epoch {ep+1}/{epochs}: Val Loss={val_loss/len(val_loader):.4f}, Val Acc={val_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\nFinal Evaluation Metrics:\")\n",
        "    print(confusion_matrix(all_labels, all_preds))\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# 8. Main Execution Script\n",
        "if __name__ == '__main__':\n",
        "    print(\"[INFO] Preparing dataset with per-sample physics features...\")\n",
        "    full_dataset = DeepSpecDataset(root_dir, samples_per_class=10)\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=SEED, stratify=[full_dataset.labels[i] for i in indices])\n",
        "\n",
        "    print(f\"[INFO] Training samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")\n",
        "    train_dataset = Subset(full_dataset, train_idx)\n",
        "    val_dataset = Subset(full_dataset, val_idx)\n",
        "\n",
        "    print(\"[INFO] Creating data loaders...\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(\"[INFO] Initializing model and starting training...\")\n",
        "    model = DeepSPEC(num_classes=7)\n",
        "    train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(\"[INFO] Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrJennRj4OLp",
        "outputId": "ec05f47e-46af-4c74-9e1f-e3059d0e104f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['0', '1', '2', '3', '4', '5', '6']\n",
            "Class 0 has 23602 images\n",
            "Class 1 has 22009 images\n",
            "Class 2 has 21505 images\n",
            "Class 3 has 7125 images\n",
            "Class 4 has 7125 images\n",
            "Class 5 has 5281 images\n",
            "Class 6 has 5291 images\n",
            "[INFO] Preparing dataset with per-sample physics features...\n",
            "[INFO] Training samples: 56, Validation samples: 14\n",
            "[INFO] Creating data loaders...\n",
            "[INFO] Initializing model and starting training...\n",
            "Epoch 1/10: Train Loss=2.9922, Train Acc=14.29%\n",
            "Epoch 1/10: Val Loss=2.8395, Val Acc=14.29%\n",
            "Epoch 2/10: Train Loss=2.3317, Train Acc=16.07%\n",
            "Epoch 2/10: Val Loss=2.0734, Val Acc=21.43%\n",
            "Epoch 3/10: Train Loss=1.7836, Train Acc=32.14%\n",
            "Epoch 3/10: Val Loss=1.8934, Val Acc=14.29%\n",
            "Epoch 4/10: Train Loss=1.8120, Train Acc=28.57%\n",
            "Epoch 4/10: Val Loss=1.9004, Val Acc=14.29%\n",
            "Epoch 5/10: Train Loss=1.8528, Train Acc=28.57%\n",
            "Epoch 5/10: Val Loss=1.8772, Val Acc=28.57%\n",
            "Epoch 6/10: Train Loss=1.7723, Train Acc=35.71%\n",
            "Epoch 6/10: Val Loss=1.7936, Val Acc=35.71%\n",
            "Epoch 7/10: Train Loss=1.7161, Train Acc=46.43%\n",
            "Epoch 7/10: Val Loss=1.7721, Val Acc=21.43%\n",
            "Epoch 8/10: Train Loss=1.6897, Train Acc=28.57%\n",
            "Epoch 8/10: Val Loss=1.7559, Val Acc=28.57%\n",
            "Epoch 9/10: Train Loss=1.6112, Train Acc=50.00%\n",
            "Epoch 9/10: Val Loss=1.7944, Val Acc=35.71%\n",
            "Epoch 10/10: Train Loss=1.6014, Train Acc=46.43%\n",
            "Epoch 10/10: Val Loss=1.8369, Val Acc=35.71%\n",
            "\n",
            "Final Evaluation Metrics:\n",
            "[[0 0 2 0 0 0 0]\n",
            " [0 0 2 0 0 0 0]\n",
            " [0 0 2 0 0 0 0]\n",
            " [0 0 1 0 0 0 1]\n",
            " [0 0 0 0 1 0 1]\n",
            " [0 0 0 0 0 1 1]\n",
            " [0 0 0 0 1 0 1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000         2\n",
            "           1     0.0000    0.0000    0.0000         2\n",
            "           2     0.2857    1.0000    0.4444         2\n",
            "           3     0.0000    0.0000    0.0000         2\n",
            "           4     0.5000    0.5000    0.5000         2\n",
            "           5     1.0000    0.5000    0.6667         2\n",
            "           6     0.2500    0.5000    0.3333         2\n",
            "\n",
            "    accuracy                         0.3571        14\n",
            "   macro avg     0.2908    0.3571    0.2778        14\n",
            "weighted avg     0.2908    0.3571    0.2778        14\n",
            "\n",
            "[INFO] Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5XmH8u9C-0J5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}